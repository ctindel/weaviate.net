Environment Setup Instructions for Weaviate .NET Client Development

IMPORTANT NOTE FOR DEVIN:
As a Devin instance, you should perform all these setup and installation steps yourself rather than asking the user to do them. You have the necessary permissions and capabilities to execute these commands. Only ask the user for help if you encounter unexpected errors that aren't covered in these instructions.

1. .NET 8 SDK Setup
------------------
- Install .NET 8 SDK:
```bash
wget https://dot.net/v1/dotnet-install.sh
chmod +x dotnet-install.sh
./dotnet-install.sh --channel 8.0
```
- Configure environment variables:
```bash
# Set variables for current session
export DOTNET_ROOT=$HOME/.dotnet
export PATH=$PATH:$DOTNET_ROOT:$DOTNET_ROOT/tools

# Add to .bash_profile for persistence
echo 'export DOTNET_ROOT=$HOME/.dotnet' >> ~/.bash_profile
echo 'export PATH=$PATH:$DOTNET_ROOT:$DOTNET_ROOT/tools' >> ~/.bash_profile
source ~/.bash_profile
```

Note: The environment variables must be added to .bash_profile to ensure they persist across sessions. This is required for the dotnet command to work properly in all shells.

2. Weaviate Setup
----------------
- Install Docker and Docker Compose
- Create docker-compose.yml with Weaviate configuration:
```yaml
version: '3.4'
services:
  weaviate:
    image: semitechnologies/weaviate:1.28.2
    ports:
      - "8080:8080"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'text2vec-ollama'
      ENABLE_MODULES: 'text2vec-ollama,generative-ollama'
      CLUSTER_HOSTNAME: 'node1'
    volumes:
      - weaviate_data:/var/lib/weaviate
volumes:
  weaviate_data:
```
- Start Weaviate:
```bash
docker-compose up -d
```

Note: For running integration tests, use the docker-compose.yml file in the tests-integration directory:
```bash
cd tests-integration
docker-compose up -d
```
This configuration includes the necessary text2vec-ollama module and proper networking setup for test execution.

3. Ollama Setup
--------------
- Install Ollama:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```
- Start Ollama service (requires sudo):
```bash
sudo systemctl start ollama
```

Note: Starting the Ollama service requires sudo privileges. If you encounter permission issues, make sure you have the necessary sudo access.
- Pull required models:
```bash
ollama pull mxbai-embed-large
ollama pull llama3.2
```

Note: After installing .NET SDK, you must set up the environment variables in your shell session:
```bash
export DOTNET_ROOT=$HOME/.dotnet
export PATH=$PATH:$DOTNET_ROOT:$DOTNET_ROOT/tools
```
These environment variables are required for the dotnet command to work properly.
- Verify Ollama is running:
```bash
curl http://localhost:11434/api/embeddings -d '{
  "model": "mxbai-embed-large",
  "prompt": "Hello, world"
}'
```

4. Project Setup
---------------
- Clone the repository:
```bash
git clone https://github.com/ctindel/weaviate.net.git
cd weaviate.net
```
- Restore dependencies:
```bash
dotnet restore
```
- Build the project:
```bash
dotnet build
```
- Run tests:
```bash
dotnet test
```

Note: The integration tests expect Weaviate to be running on localhost:8080 and Ollama to be accessible. Due to Docker networking:
- For local development and testing from your machine, use "localhost:11434" as the Ollama endpoint
- For Docker containers (like Weaviate) connecting to Ollama, use "host.docker.internal:11434" as the endpoint

This is particularly important in the vectorizer configuration when creating collections. Make sure to use the appropriate endpoint URL based on the context:
- Use "http://localhost:11434" when making direct API calls to Ollama
- Use "http://host.docker.internal:11434" in collection configurations that will be used by Weaviate

The mxbai-embed-large and llama3.2 models must be available through Ollama for the tests to work properly.

Important: When creating collections with text2vec-ollama vectorizer, the configuration must follow this structure:
```json
{
  "class": "Collection",
  "vectorizer": "text2vec-ollama",
  "vectorIndexType": "hnsw",
  "vectorIndexConfig": {
    "distance": "cosine"
  },
  "vectorizerConfig": {
    "model": "mxbai-embed-large",
    "api_endpoint": "http://host.docker.internal:11434"
  }
}
```
Note the distinction between "vectorIndexConfig" and "vectorizerConfig". The vectorizer settings must be under "vectorizerConfig", not inside "moduleConfig" or other configuration blocks.
